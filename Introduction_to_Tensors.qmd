---
title: "Introduction to Tensors"
format: html
execute: 
  eval: false
---

## Tensors

In the context of mathematics, physics, and computer science, a tensor is a mathematical object that generalizes the concept of vectors and matrices. Tensors can be thought of as multi-dimensional arrays or data structures that represent data in a way that is suitable for certain operations.


- A tensor's order (or rank) defines the number of dimensions it has. For example:
    - A scalar (0th-order tensor) is a single value.
    - A vector (1st-order tensor) is an array of values.
    - A matrix (2nd-order tensor) is a 2D array of values.
    - Tensors with three or more dimensions are higher-order tensors.

## Derivatives


- $y = f(x = 2) = x ^ 2$
- $\frac{d}{dx}f(x = 2) = 2x = 4$

```{python}
import torch

x = torch.tensor(2.0, requires_grad=True)
y = x ** 2
y.backward()
print(x.grad)
```

## Partial Derivatives

- $y = f(u,v) = uv + u^2$
- $\frac{d}{du}f(u,v) = v + 2u$
- $\frac{d}{dv}f(u,v) = u$


```{python}

u = torch.tensor(1.0, requires_grad=True)
v = torch.tensor(2.0, requires_grad=True)
y = u*v + u **2
y.backward()
print(u.grad)
print(v.grad)

```

## Linear Regression Problem (Simple Linear Regression)
   - Linear regression is a supervised learning algorithm used for predicting a continuous outcome.
   - It assumes a linear relationship between the independent variable(s) and the dependent variable.
   - The linear regression equation: $y = mx + b$, where $m$ is the slope and $b$ is the intercept.

```{python}
from torch.nn import Linear

torch.manual_seed(1)
mdl = Linear(in_features=1, out_features=1)
print(list(mdl.parameters()))
x = torch.tensor([[0.0]])
yhat = mdl(x)
print(yhat)

```

## Custom Model


```{python}
import torch.nn as nn

class LR(nn.Module):
    def __init__(self, in_size, out_size):
        super(LR, self).__init__()
        self.linear = nn.Linear(in_size,out_size)
    
    def forward(self, x):
        out = self.linear(x)
        return out
```
The provided code is a simple implementation of linear regression using PyTorch. Let's break down the code step by step:

Here, a class named `LR` (Linear Regression) is defined, which is a subclass of `nn.Module`. In PyTorch, neural network models are typically defined as classes that inherit from `nn.Module`. The `__init__` method is used to initialize the model.

- `in_size`: Specifies the size of the input features (number of input neurons).
- `out_size`: Specifies the size of the output (number of output neurons).

Inside the `__init__` method:

- `super(LR, self).__init__()` calls the constructor of the parent class (`nn.Module`) to initialize the module.
- `self.linear = nn.Linear(in_size, out_size)` creates a linear layer. The `nn.Linear` module represents a linear transformation, which includes weights and biases. It takes `in_size` as the number of input features and `out_size` as the number of output features.

The `forward` method defines the forward pass of the model. In PyTorch, the `forward` method is where you specify how input data should be passed through the network. Here:

- `x` is the input data.
- `self.linear(x)` applies the linear transformation to the input, which essentially computes \(wx + b\), where \(w\) is the weight matrix and \(b\) is the bias vector.
- The result is stored in the variable `out`.
- The method returns `out`, which represents the output of the linear regression model.


```{python}
lr = LR(1, 1)

X= torch.tensor([[1.0], [2.0], [3.0]])

yhat=lr(X)
print(yhat)
```

## Training the Model

- $D = {(x_1, y_1), ... , (x_N, y_N)}$
- $\hat{y} = wx + b$

## Gradient Descent

## Gradient Descent Algorithm

$$ L =  \frac{1}{2n}\sum_{i = 1}^n \left({y^i - \hat{y}^i}\right)^2 $$

```{python}
#| echo: false

import matplotlib.pyplot as plt
import numpy as np
# Define the quadratic function f(x) = ax^2 + bx + c
def quadratic_function(x, a, b, c):
    return a * x**2 + b * x + c

# Define the derivative of the quadratic function
def derivative_quadratic(x, a, b):
    return 2 * a * x + b

# Define the coefficients of the quadratic function
a = 1
b = -2
c = 1

# Choose the point (x0, y0) where you want to draw the tangent line
x0 = 2.5
y0 = quadratic_function(x0, a, b, c)

# Calculate the slope of the tangent line
m = derivative_quadratic(x0, a, b)

# Calculate the y-intercept of the tangent line
b_tangent = y0 - m * x0

# Create a range of x-values for the tangent line
x_tangent = np.linspace(-1.5, 3.5, 100)

# Calculate the corresponding y-values for the tangent line
y_tangent = m * x_tangent + b_tangent

# Create a range of x-values for the quadratic function
x = np.linspace(-1.5, 3.5, 100)

# Calculate y-values for the quadratic function
y = quadratic_function(x, a, b, c)

# Plot the quadratic function and the tangent line
plt.plot(x, y, label='Loss Function')
plt.plot(x_tangent, y_tangent, label='Tangent Line', linestyle='--')
plt.scatter(x0, y0, color='red', label='Point of Tangency')
plt.xlabel(r'$\beta$ ')
plt.ylabel('Loss')
plt.legend()
plt.ylim([-1,6.5])
plt.xticks([])
plt.yticks([])
plt.grid()
plt.show()
```

## Gradient Descent Algorithm

$$\beta^{\text{new}} = \beta^{\text{old}} - \alpha\frac{\partial L}{\partial \beta}$$

\begin{aligned}

\frac{\partial L}{\partial \beta_1} & = \frac{\partial}{\partial \beta_1} \frac{1}{2n}\sum_{i = 1}^n \left({y^i - \hat{y}^i}\right)^2 \\

& = \frac{1}{2n}\sum_{i = 1}^n \frac{\partial}{\partial \beta_1}  \left({y^i - \hat{y}^i}\right)^2 \\

& = \frac{1}{2n}\sum_{i = 1}^n \frac{\partial}{\partial \beta_1}  \left({y^i - \beta_0 - \beta_1 x^i}\right)^2 \\

\end{aligned}

## Taking Derivatives

\begin{aligned}

\frac{\partial L}{\partial \beta_1} & = \frac{1}{2n}\sum_{i = 1}^n \frac{\partial}{\partial \beta_1}  \left({y^i - \beta_0 - \beta_1 x^i}\right)^2 \\

& = \frac{1}{2n} * 2 \sum_{i = 1}^n \left({y^i - \beta_0 - \beta_1 x^i}\right) \left({-x^i}\right) \\

& = \frac{1}{n} \sum_{i = 1}^n \left({e^i}\right) \left({-x^i}\right) \\

\end{aligned}

## Taking Derivatives
\begin{aligned}

\frac{\partial L}{\partial \beta_0} & = \frac{1}{2n}\sum_{i = 1}^n \frac{\partial}{\partial \beta_0}  \left({y^i - \beta_0 - \beta_1 x^i}\right)^2 \\

& = \frac{1}{2n} * 2 \sum_{i = 1}^n \left({y^i - \beta_0 - \beta_1 x^i}\right) \left({-1}\right) \\

& = \frac{1}{n} \sum_{i = 1}^n \left({e^i}\right) \left({-1}\right) \\

\end{aligned}

## Updating Derivatives

$$\beta^{\text{new}} = \beta^{\text{old}} - \alpha\frac{\partial L}{\partial \beta}$$

$$\beta_1^{\text{new}} = \beta_1^{\text{old}} - \alpha\frac{\partial L}{\partial \beta_1}$$

$$\beta_0^{\text{new}} = \beta_0^{\text{old}} - \alpha\frac{\partial L}{\partial \beta_0}$$

## PyTorch example

### Initialize Data

```{python}
import torch
import matplotlib.pyplot as plt

w = torch.tensor(-5.0, requires_grad=True)
b = torch.tensor(0.0, requires_grad=True)
X = torch.linspace(-3,3,steps = 10).view(-1,1)
y = -3 * X + 2 + torch.randn(size = X.size())

plt.plot(X.numpy(), y.numpy(),"ob")
plt.grid()

```

### Perform Prediction


```{python}
def forward(x):
    return w*x + b

def criterion(yhat, y):
    return torch.mean((yhat-y)**2)

ypred = forward(X)
print(criterion(ypred, y))

plt.plot(X.numpy(), y.numpy(),"ob")
plt.plot(X.numpy(), ypred.data.numpy(),"-r")
plt.grid()
```

### Gradient Descent

```{python}
lr = 0.1
cost = []
for i in range(10):
    ypred = forward(X)
    loss = criterion(ypred, y)
    loss.backward()
    w.data = w.data - lr*w.grad.data
    b.data = b.data - lr*b.grad.data
    w.grad.data.zero_()
    b.grad.data.zero_()
    cost.append(loss.item())

print(loss)
print(w.grad)
print(b.grad)
plt.plot(X.numpy(), y.numpy(),"ob")
plt.plot(X.numpy(), ypred.data.numpy(),"-r")
plt.grid()

plt.figure()
plt.plot(cost, "-*b")
plt.grid()
plt.xlabel("Iteration")
plt.ylabel("Loss value")
```

### Stochastic Gradient Descent


```{python}
lr = 0.01
cost = []
for i in range(10):
    total = 0 
    for x_,y_ in zip(X,y):
        ypred = forward(x_)
        loss = criterion(ypred, y_)
        loss.backward()
        w.data = w.data - lr*w.grad.data
        b.data = b.data - lr*b.grad.data
        w.grad.data.zero_()
        b.grad.data.zero_()
        total += loss.item()
    cost.append(total)
print(loss)
print(w.grad)
print(b.grad)

plt.figure()
plt.plot(cost, "-*b")
plt.grid()
plt.xlabel("Iteration")
plt.ylabel("Loss value")
```

### Dataset Loader Module

```{python}
from torch.utils.data import Dataset, DataLoader

class Data(Dataset):
    def __init__(self, X, y):
        self.x = X
        self.y = y
        self.len = X.shape[0]
    
    def __getitem__(self, index):
        return self.x[index], self.y[index]

    def __len__(self):
        return self.len

dataset = Data(X,y)
x,y = dataset[:3]
print(x)
print(y)
```

### MiniBatch Gradient Descent


```{python}
trainloader = DataLoader(dataset=dataset, batch_size=5)
lr = 0.1
cost = []
for i in range(10):
    total = 0 
    for x_,y_ in trainloader:
        ypred = forward(x_)
        loss = criterion(ypred, y_)
        loss.backward()
        w.data = w.data - lr*w.grad.data
        b.data = b.data - lr*b.grad.data
        w.grad.data.zero_()
        b.grad.data.zero_()
        total += loss.item()
    cost.append(total)
print(loss)
print(w.grad)
print(b.grad)


plt.figure()
plt.plot(cost, "-*b")
plt.grid()
plt.xlabel("Iteration")
plt.ylabel("Loss value")

```