---
title: "Convolution Neural Networks"
format: html
execute: 
  echo: true
  eval: true

embed-resources: true
---


## Introduction:*

* CNNs excel at tasks like image recognition and processing due to their ability to extract features from spatial data.
* Traditional image processing treated images as flattened vectors, ignoring spatial relationships between pixels.
* This approach is inefficient and doesn't leverage the inherent structure of images.
* Convolutional Neural Networks (CNNs) address this issue by explicitly considering spatial relationships.

## Hubel and Wiesel's Work on Vision

Hubel and Wiesel's contributions to visual neuroscience are monumental, paving the way for our understanding of how the brain processes visual information. Here's a deeper look:

* **Receptive fields:** They discovered that individual neurons in the visual cortex respond to specific regions of the visual field, called receptive fields. These fields could be simple (responding to edges or lines) or complex (responding to specific shapes or movements).
* **Feature detection:** They demonstrated that neurons progressively build more complex representations of visual stimuli by integrating information from lower-level neurons. This hierarchy of feature detection leads to the perception of objects and scenes.
* **Ocular dominance columns:** They revealed that the visual cortex is organized into columns where neurons are preferentially activated by one eye or the other. This organization is crucial for binocular vision and depth perception.
* **Critical period for development:** They showed that depriving kittens of visual experience during a specific window can have profound and permanent effects on their visual development. This discovery highlighted the importance of early sensory experience for brain development.

[![Hubel and Wiesel Experiment](https://img.youtube.com/vi/IOHayh06LJ4/default.jpg)](https://www.youtube.com/watch?v=IOHayh06LJ4)



## What is Convolution?

* Convolution helps overcome the problem of shifted features in images.
* It compares the image with a smaller matrix called a kernel, focusing on relative positions of pixels.
* The output of convolution is an activation map, capturing the presence of features detected by the kernel.

![](/images/Invariance.PNG)

## Example of Convolution in PyTorch:

```{python}
import torch 
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from scipy import ndimage, misc

conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)
conv
```

```{python}
conv.state_dict()['weight'][0][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])
conv.state_dict()['bias'][0]=0.0
conv.state_dict()
```


```{python}
image=torch.zeros(1,1,5,5)
image[0,0,:,2]=1
image
```

```{python}
z=conv(image)
z
```

<img src="https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.1convltuon.gif" width="500," align="center">

## Determining Activation Map Size:

* The size of the activation map depends on:
    * *M:* Image size
    * *K:* Kernel size
    * *Stride:* Amount the kernel moves per iteration (default: 1)
* Formula: *N = M - K + 1* 



## Stride:

* Stride controls the movement of the kernel across the image.
* A stride of 1 means the kernel moves one pixel at a time.
* Larger strides reduce the output size .

<img src="https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.1stride2.gif" width="500," align="center">

## Zero Paddding

- As you apply successive convolutions, the image will shrink. You can apply zero padding to keep the image at a reasonable size, which also holds information at the borders.


```{python}
image=torch.ones(1,1,4,4)
conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3,padding=1)

conv.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])
conv.state_dict()['bias'][0]=0.0
conv.state_dict()
z=conv(image)
print("z:",z)
print("z:",z.shape[2:4])
```

<img src="https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.1zeropad.gif" width="500," align="center">


## Convolutin Neural Networks


```{python}
import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pylab as plt
import numpy as np
```


### Load Data


```{python}
IMAGE_SIZE = 16


composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])
train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)
validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)
```

### Create the model


```{python}
#| eval: false

class CNN(nn.Module):
    
    # Contructor
    def __init__(self, out_1=16, out_2=32):
        super(CNN, self).__init__()
        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)
        self.maxpool1=nn.MaxPool2d(kernel_size=2)

        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)
        self.maxpool2=nn.MaxPool2d(kernel_size=2)
        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)
    
    # Prediction
    def forward(self, x):
        x = self.cnn1(x)
        x = torch.relu(x)
        x = self.maxpool1(x)
        x = self.cnn2(x)
        x = torch.relu(x)
        x = self.maxpool2(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x
    
    # Outputs in each steps
    def activations(self, x):
        #outputs activation this is not necessary
        z1 = self.cnn1(x)
        a1 = torch.relu(z1)
        out = self.maxpool1(a1)
        
        z2 = self.cnn2(out)
        a2 = torch.relu(z2)
        out1 = self.maxpool2(a2)
        out = out.view(out.size(0),-1)
        return z1, a1, z2, a2, out1,out

model = CNN(out_1=16, out_2=32)
```

### Train the model


```{python}
#| eval: false


criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)

n_epochs=3
cost_list=[]
accuracy_list=[]
N_test=len(validation_dataset)
COST=0

def train_model(n_epochs):
    for epoch in range(n_epochs):
        COST=0
        for x, y in train_loader:
            optimizer.zero_grad()
            z = model(x)
            loss = criterion(z, y)
            loss.backward()
            optimizer.step()
            COST+=loss.data
        
        cost_list.append(COST)
        correct=0
        #perform a prediction on the validation  data  
        for x_test, y_test in validation_loader:
            z = model(x_test)
            _, yhat = torch.max(z.data, 1)
            correct += (yhat == y_test).sum().item()
        accuracy = correct / N_test
        accuracy_list.append(accuracy)
     
train_model(n_epochs)

```

## Create dataset from local images

