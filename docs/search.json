[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "VIA528E",
    "section": "",
    "text": "Chapter 2 Introduction to PyTorch"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "Introduction to Tensors.html",
    "href": "Introduction to Tensors.html",
    "title": "Untitled",
    "section": "",
    "text": "In the context of mathematics, physics, and computer science, a tensor is a mathematical object that generalizes the concept of vectors and matrices. Tensors can be thought of as multi-dimensional arrays or data structures that represent data in a way that is suitable for certain operations.\n\nA tensor’s order (or rank) defines the number of dimensions it has. For example:\n\nA scalar (0th-order tensor) is a single value.\nA vector (1st-order tensor) is an array of values.\nA matrix (2nd-order tensor) is a 2D array of values.\nTensors with three or more dimensions are higher-order tensors."
  },
  {
    "objectID": "Introduction to Tensors.html#tensors",
    "href": "Introduction to Tensors.html#tensors",
    "title": "Untitled",
    "section": "",
    "text": "In the context of mathematics, physics, and computer science, a tensor is a mathematical object that generalizes the concept of vectors and matrices. Tensors can be thought of as multi-dimensional arrays or data structures that represent data in a way that is suitable for certain operations.\n\nA tensor’s order (or rank) defines the number of dimensions it has. For example:\n\nA scalar (0th-order tensor) is a single value.\nA vector (1st-order tensor) is an array of values.\nA matrix (2nd-order tensor) is a 2D array of values.\nTensors with three or more dimensions are higher-order tensors."
  },
  {
    "objectID": "Introduction to Tensors.html#derivatives",
    "href": "Introduction to Tensors.html#derivatives",
    "title": "Untitled",
    "section": "Derivatives",
    "text": "Derivatives\n\n\\(y = f(x = 2) = x ^ 2\\)\n\\(\\frac{d}{dx}f(x = 2) = 2x = 4\\)\n\n\nimport torch\n\nx = torch.tensor(2.0, requires_grad=True)\ny = x ** 2\ny.backward()\nprint(x.grad)"
  },
  {
    "objectID": "Introduction to Tensors.html#partial-derivatives",
    "href": "Introduction to Tensors.html#partial-derivatives",
    "title": "Untitled",
    "section": "Partial Derivatives",
    "text": "Partial Derivatives\n\n\\(y = f(u,v) = uv + u^2\\)\n\\(\\frac{d}{du}f(u,v) = v + 2u\\)\n\\(\\frac{d}{dv}f(u,v) = u\\)\n\n\nu = torch.tensor(1.0, requires_grad=True)\nv = torch.tensor(2.0, requires_grad=True)\ny = u*v + u **2\ny.backward()\nprint(u.grad)\nprint(v.grad)"
  },
  {
    "objectID": "Introduction to Tensors.html#linear-regression-problem-simple-linear-regression",
    "href": "Introduction to Tensors.html#linear-regression-problem-simple-linear-regression",
    "title": "Untitled",
    "section": "Linear Regression Problem (Simple Linear Regression)",
    "text": "Linear Regression Problem (Simple Linear Regression)\n\nLinear regression is a supervised learning algorithm used for predicting a continuous outcome.\nIt assumes a linear relationship between the independent variable(s) and the dependent variable.\nThe linear regression equation: \\(y = mx + b\\), where \\(m\\) is the slope and \\(b\\) is the intercept.\n\n\nfrom torch.nn import Linear\n\ntorch.manual_seed(1)\nmdl = Linear(in_features=1, out_features=1)\nprint(list(mdl.parameters()))\nx = torch.tensor([[0.0]])\nyhat = mdl(x)\nprint(yhat)"
  },
  {
    "objectID": "Introduction to Tensors.html#custom-model",
    "href": "Introduction to Tensors.html#custom-model",
    "title": "Untitled",
    "section": "Custom Model",
    "text": "Custom Model\n\nimport torch.nn as nn\n\nclass LR(nn.Module):\n    def __init__(self, in_size, out_size):\n        super(LR, self).__init__()\n        self.linear = nn.Linear(in_size,out_size)\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\nThe provided code is a simple implementation of linear regression using PyTorch. Let’s break down the code step by step:\nHere, a class named LR (Linear Regression) is defined, which is a subclass of nn.Module. In PyTorch, neural network models are typically defined as classes that inherit from nn.Module. The __init__ method is used to initialize the model.\n\nin_size: Specifies the size of the input features (number of input neurons).\nout_size: Specifies the size of the output (number of output neurons).\n\nInside the __init__ method:\n\nsuper(LR, self).__init__() calls the constructor of the parent class (nn.Module) to initialize the module.\nself.linear = nn.Linear(in_size, out_size) creates a linear layer. The nn.Linear module represents a linear transformation, which includes weights and biases. It takes in_size as the number of input features and out_size as the number of output features.\n\nThe forward method defines the forward pass of the model. In PyTorch, the forward method is where you specify how input data should be passed through the network. Here:\n\nx is the input data.\nself.linear(x) applies the linear transformation to the input, which essentially computes (wx + b), where (w) is the weight matrix and (b) is the bias vector.\nThe result is stored in the variable out.\nThe method returns out, which represents the output of the linear regression model.\n\n\nlr = LR(1, 1)\n\nX= torch.tensor([[1.0], [2.0], [3.0]])\n\nyhat=lr(X)\nprint(yhat)"
  },
  {
    "objectID": "Introduction to Tensors.html#training-the-model",
    "href": "Introduction to Tensors.html#training-the-model",
    "title": "Untitled",
    "section": "Training the Model",
    "text": "Training the Model\n\n\\(D = {(x_1, y_1), ... , (x_N, y_N)}\\)\n\\(\\hat{y} = wx + b\\)"
  },
  {
    "objectID": "Introduction to Tensors.html#gradient-descent",
    "href": "Introduction to Tensors.html#gradient-descent",
    "title": "Untitled",
    "section": "Gradient Descent",
    "text": "Gradient Descent"
  },
  {
    "objectID": "Introduction to Tensors.html#gradient-descent-algorithm",
    "href": "Introduction to Tensors.html#gradient-descent-algorithm",
    "title": "Untitled",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\\[ L =  \\frac{1}{2n}\\sum_{i = 1}^n \\left({y^i - \\hat{y}^i}\\right)^2 \\]"
  },
  {
    "objectID": "Introduction to Tensors.html#gradient-descent-algorithm-1",
    "href": "Introduction to Tensors.html#gradient-descent-algorithm-1",
    "title": "Untitled",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\\[\\beta^{\\text{new}} = \\beta^{\\text{old}} - \\alpha\\frac{\\partial L}{\\partial \\beta}\\]\n\\[\\begin{aligned}\n\n\\frac{\\partial L}{\\partial \\beta_1} & = \\frac{\\partial}{\\partial \\beta_1} \\frac{1}{2n}\\sum_{i = 1}^n \\left({y^i - \\hat{y}^i}\\right)^2 \\\\\n\n& = \\frac{1}{2n}\\sum_{i = 1}^n \\frac{\\partial}{\\partial \\beta_1}  \\left({y^i - \\hat{y}^i}\\right)^2 \\\\\n\n& = \\frac{1}{2n}\\sum_{i = 1}^n \\frac{\\partial}{\\partial \\beta_1}  \\left({y^i - \\beta_0 - \\beta_1 x^i}\\right)^2 \\\\\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "Introduction to Tensors.html#taking-derivatives",
    "href": "Introduction to Tensors.html#taking-derivatives",
    "title": "Untitled",
    "section": "Taking Derivatives",
    "text": "Taking Derivatives\n\\[\\begin{aligned}\n\n\\frac{\\partial L}{\\partial \\beta_1} & = \\frac{1}{2n}\\sum_{i = 1}^n \\frac{\\partial}{\\partial \\beta_1}  \\left({y^i - \\beta_0 - \\beta_1 x^i}\\right)^2 \\\\\n\n& = \\frac{1}{2n} * 2 \\sum_{i = 1}^n \\left({y^i - \\beta_0 - \\beta_1 x^i}\\right) \\left({-x^i}\\right) \\\\\n\n& = \\frac{1}{n} \\sum_{i = 1}^n \\left({e^i}\\right) \\left({-x^i}\\right) \\\\\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "Introduction to Tensors.html#taking-derivatives-1",
    "href": "Introduction to Tensors.html#taking-derivatives-1",
    "title": "Untitled",
    "section": "Taking Derivatives",
    "text": "Taking Derivatives\n\\[\\begin{aligned}\n\n\\frac{\\partial L}{\\partial \\beta_0} & = \\frac{1}{2n}\\sum_{i = 1}^n \\frac{\\partial}{\\partial \\beta_0}  \\left({y^i - \\beta_0 - \\beta_1 x^i}\\right)^2 \\\\\n\n& = \\frac{1}{2n} * 2 \\sum_{i = 1}^n \\left({y^i - \\beta_0 - \\beta_1 x^i}\\right) \\left({-1}\\right) \\\\\n\n& = \\frac{1}{n} \\sum_{i = 1}^n \\left({e^i}\\right) \\left({-1}\\right) \\\\\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "Introduction to Tensors.html#updating-derivatives",
    "href": "Introduction to Tensors.html#updating-derivatives",
    "title": "Untitled",
    "section": "Updating Derivatives",
    "text": "Updating Derivatives\n\\[\\beta^{\\text{new}} = \\beta^{\\text{old}} - \\alpha\\frac{\\partial L}{\\partial \\beta}\\]\n\\[\\beta_1^{\\text{new}} = \\beta_1^{\\text{old}} - \\alpha\\frac{\\partial L}{\\partial \\beta_1}\\]\n\\[\\beta_0^{\\text{new}} = \\beta_0^{\\text{old}} - \\alpha\\frac{\\partial L}{\\partial \\beta_0}\\]"
  },
  {
    "objectID": "Introduction to Tensors.html#pytorch-example",
    "href": "Introduction to Tensors.html#pytorch-example",
    "title": "Untitled",
    "section": "PyTorch example",
    "text": "PyTorch example\n\nInitialize Data\n\nimport torch\nimport matplotlib.pyplot as plt\n\nw = torch.tensor(-5.0, requires_grad=True)\nb = torch.tensor(0.0, requires_grad=True)\nX = torch.linspace(-3,3,steps = 10).view(-1,1)\ny = -3 * X + 2 + torch.randn(size = X.size())\n\nplt.plot(X.numpy(), y.numpy(),\"ob\")\nplt.grid()\n\n\n\nPerform Prediction\n\ndef forward(x):\n    return w*x + b\n\ndef criterion(yhat, y):\n    return torch.mean((yhat-y)**2)\n\nypred = forward(X)\nprint(criterion(ypred, y))\n\nplt.plot(X.numpy(), y.numpy(),\"ob\")\nplt.plot(X.numpy(), ypred.data.numpy(),\"-r\")\nplt.grid()\n\n\n\nGradient Descent\n\nlr = 0.1\ncost = []\nfor i in range(10):\n    ypred = forward(X)\n    loss = criterion(ypred, y)\n    loss.backward()\n    w.data = w.data - lr*w.grad.data\n    b.data = b.data - lr*b.grad.data\n    w.grad.data.zero_()\n    b.grad.data.zero_()\n    cost.append(loss.item())\n\nprint(loss)\nprint(w.grad)\nprint(b.grad)\nplt.plot(X.numpy(), y.numpy(),\"ob\")\nplt.plot(X.numpy(), ypred.data.numpy(),\"-r\")\nplt.grid()\n\nplt.figure()\nplt.plot(cost, \"-*b\")\nplt.grid()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss value\")\n\n\n\nStochastic Gradient Descent\n\nlr = 0.01\ncost = []\nfor i in range(10):\n    total = 0 \n    for x_,y_ in zip(X,y):\n        ypred = forward(x_)\n        loss = criterion(ypred, y_)\n        loss.backward()\n        w.data = w.data - lr*w.grad.data\n        b.data = b.data - lr*b.grad.data\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n        total += loss.item()\n    cost.append(total)\nprint(loss)\nprint(w.grad)\nprint(b.grad)\n\nplt.figure()\nplt.plot(cost, \"-*b\")\nplt.grid()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss value\")\n\n\n\nDataset Loader Module\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Data(Dataset):\n    def __init__(self, X, y):\n        self.x = X\n        self.y = y\n        self.len = X.shape[0]\n    \n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n\n    def __len__(self):\n        return self.len\n\ndataset = Data(X,y)\nx,y = dataset[:3]\nprint(x)\nprint(y)\n\n\n\nMiniBatch Gradient Descent\n\ntrainloader = DataLoader(dataset=dataset, batch_size=5)\nlr = 0.1\ncost = []\nfor i in range(10):\n    total = 0 \n    for x_,y_ in trainloader:\n        ypred = forward(x_)\n        loss = criterion(ypred, y_)\n        loss.backward()\n        w.data = w.data - lr*w.grad.data\n        b.data = b.data - lr*b.grad.data\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n        total += loss.item()\n    cost.append(total)\nprint(loss)\nprint(w.grad)\nprint(b.grad)\n\n\nplt.figure()\nplt.plot(cost, \"-*b\")\nplt.grid()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss value\")"
  },
  {
    "objectID": "Introduction_to_Tensors.html",
    "href": "Introduction_to_Tensors.html",
    "title": "Introduction to Tensors",
    "section": "",
    "text": "In the context of mathematics, physics, and computer science, a tensor is a mathematical object that generalizes the concept of vectors and matrices. Tensors can be thought of as multi-dimensional arrays or data structures that represent data in a way that is suitable for certain operations.\n\nA tensor’s order (or rank) defines the number of dimensions it has. For example:\n\nA scalar (0th-order tensor) is a single value.\nA vector (1st-order tensor) is an array of values.\nA matrix (2nd-order tensor) is a 2D array of values.\nTensors with three or more dimensions are higher-order tensors."
  },
  {
    "objectID": "Introduction_to_Tensors.html#tensors",
    "href": "Introduction_to_Tensors.html#tensors",
    "title": "Introduction to Tensors",
    "section": "",
    "text": "In the context of mathematics, physics, and computer science, a tensor is a mathematical object that generalizes the concept of vectors and matrices. Tensors can be thought of as multi-dimensional arrays or data structures that represent data in a way that is suitable for certain operations.\n\nA tensor’s order (or rank) defines the number of dimensions it has. For example:\n\nA scalar (0th-order tensor) is a single value.\nA vector (1st-order tensor) is an array of values.\nA matrix (2nd-order tensor) is a 2D array of values.\nTensors with three or more dimensions are higher-order tensors."
  },
  {
    "objectID": "Introduction_to_Tensors.html#derivatives",
    "href": "Introduction_to_Tensors.html#derivatives",
    "title": "Introduction to Tensors",
    "section": "Derivatives",
    "text": "Derivatives\n\n\\(y = f(x = 2) = x ^ 2\\)\n\\(\\frac{d}{dx}f(x = 2) = 2x = 4\\)\n\n\nimport torch\n\nx = torch.tensor(2.0, requires_grad=True)\ny = x ** 2\ny.backward()\nprint(x.grad)"
  },
  {
    "objectID": "Introduction_to_Tensors.html#partial-derivatives",
    "href": "Introduction_to_Tensors.html#partial-derivatives",
    "title": "Introduction to Tensors",
    "section": "Partial Derivatives",
    "text": "Partial Derivatives\n\n\\(y = f(u,v) = uv + u^2\\)\n\\(\\frac{d}{du}f(u,v) = v + 2u\\)\n\\(\\frac{d}{dv}f(u,v) = u\\)\n\n\nu = torch.tensor(1.0, requires_grad=True)\nv = torch.tensor(2.0, requires_grad=True)\ny = u*v + u **2\ny.backward()\nprint(u.grad)\nprint(v.grad)"
  },
  {
    "objectID": "Introduction_to_Tensors.html#linear-regression-problem-simple-linear-regression",
    "href": "Introduction_to_Tensors.html#linear-regression-problem-simple-linear-regression",
    "title": "Introduction to Tensors",
    "section": "Linear Regression Problem (Simple Linear Regression)",
    "text": "Linear Regression Problem (Simple Linear Regression)\n\nLinear regression is a supervised learning algorithm used for predicting a continuous outcome.\nIt assumes a linear relationship between the independent variable(s) and the dependent variable.\nThe linear regression equation: \\(y = mx + b\\), where \\(m\\) is the slope and \\(b\\) is the intercept.\n\n\nfrom torch.nn import Linear\n\ntorch.manual_seed(1)\nmdl = Linear(in_features=1, out_features=1)\nprint(list(mdl.parameters()))\nx = torch.tensor([[0.0]])\nyhat = mdl(x)\nprint(yhat)"
  },
  {
    "objectID": "Introduction_to_Tensors.html#custom-model",
    "href": "Introduction_to_Tensors.html#custom-model",
    "title": "Introduction to Tensors",
    "section": "Custom Model",
    "text": "Custom Model\n\nimport torch.nn as nn\n\nclass LR(nn.Module):\n    def __init__(self, in_size, out_size):\n        super(LR, self).__init__()\n        self.linear = nn.Linear(in_size,out_size)\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\nThe provided code is a simple implementation of linear regression using PyTorch. Let’s break down the code step by step:\nHere, a class named LR (Linear Regression) is defined, which is a subclass of nn.Module. In PyTorch, neural network models are typically defined as classes that inherit from nn.Module. The __init__ method is used to initialize the model.\n\nin_size: Specifies the size of the input features (number of input neurons).\nout_size: Specifies the size of the output (number of output neurons).\n\nInside the __init__ method:\n\nsuper(LR, self).__init__() calls the constructor of the parent class (nn.Module) to initialize the module.\nself.linear = nn.Linear(in_size, out_size) creates a linear layer. The nn.Linear module represents a linear transformation, which includes weights and biases. It takes in_size as the number of input features and out_size as the number of output features.\n\nThe forward method defines the forward pass of the model. In PyTorch, the forward method is where you specify how input data should be passed through the network. Here:\n\nx is the input data.\nself.linear(x) applies the linear transformation to the input, which essentially computes (wx + b), where (w) is the weight matrix and (b) is the bias vector.\nThe result is stored in the variable out.\nThe method returns out, which represents the output of the linear regression model.\n\n\nlr = LR(1, 1)\n\nX= torch.tensor([[1.0], [2.0], [3.0]])\n\nyhat=lr(X)\nprint(yhat)"
  },
  {
    "objectID": "Introduction_to_Tensors.html#training-the-model",
    "href": "Introduction_to_Tensors.html#training-the-model",
    "title": "Introduction to Tensors",
    "section": "Training the Model",
    "text": "Training the Model\n\n\\(D = {(x_1, y_1), ... , (x_N, y_N)}\\)\n\\(\\hat{y} = wx + b\\)"
  },
  {
    "objectID": "Introduction_to_Tensors.html#gradient-descent",
    "href": "Introduction_to_Tensors.html#gradient-descent",
    "title": "Introduction to Tensors",
    "section": "Gradient Descent",
    "text": "Gradient Descent"
  },
  {
    "objectID": "Introduction_to_Tensors.html#gradient-descent-algorithm",
    "href": "Introduction_to_Tensors.html#gradient-descent-algorithm",
    "title": "Introduction to Tensors",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\\[ L =  \\frac{1}{2n}\\sum_{i = 1}^n \\left({y^i - \\hat{y}^i}\\right)^2 \\]"
  },
  {
    "objectID": "Introduction_to_Tensors.html#gradient-descent-algorithm-1",
    "href": "Introduction_to_Tensors.html#gradient-descent-algorithm-1",
    "title": "Introduction to Tensors",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\\[\\beta^{\\text{new}} = \\beta^{\\text{old}} - \\alpha\\frac{\\partial L}{\\partial \\beta}\\]\n\\[\\begin{aligned}\n\n\\frac{\\partial L}{\\partial \\beta_1} & = \\frac{\\partial}{\\partial \\beta_1} \\frac{1}{2n}\\sum_{i = 1}^n \\left({y^i - \\hat{y}^i}\\right)^2 \\\\\n\n& = \\frac{1}{2n}\\sum_{i = 1}^n \\frac{\\partial}{\\partial \\beta_1}  \\left({y^i - \\hat{y}^i}\\right)^2 \\\\\n\n& = \\frac{1}{2n}\\sum_{i = 1}^n \\frac{\\partial}{\\partial \\beta_1}  \\left({y^i - \\beta_0 - \\beta_1 x^i}\\right)^2 \\\\\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "Introduction_to_Tensors.html#taking-derivatives",
    "href": "Introduction_to_Tensors.html#taking-derivatives",
    "title": "Introduction to Tensors",
    "section": "Taking Derivatives",
    "text": "Taking Derivatives\n\\[\\begin{aligned}\n\n\\frac{\\partial L}{\\partial \\beta_1} & = \\frac{1}{2n}\\sum_{i = 1}^n \\frac{\\partial}{\\partial \\beta_1}  \\left({y^i - \\beta_0 - \\beta_1 x^i}\\right)^2 \\\\\n\n& = \\frac{1}{2n} * 2 \\sum_{i = 1}^n \\left({y^i - \\beta_0 - \\beta_1 x^i}\\right) \\left({-x^i}\\right) \\\\\n\n& = \\frac{1}{n} \\sum_{i = 1}^n \\left({e^i}\\right) \\left({-x^i}\\right) \\\\\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "Introduction_to_Tensors.html#taking-derivatives-1",
    "href": "Introduction_to_Tensors.html#taking-derivatives-1",
    "title": "Introduction to Tensors",
    "section": "Taking Derivatives",
    "text": "Taking Derivatives\n\\[\\begin{aligned}\n\n\\frac{\\partial L}{\\partial \\beta_0} & = \\frac{1}{2n}\\sum_{i = 1}^n \\frac{\\partial}{\\partial \\beta_0}  \\left({y^i - \\beta_0 - \\beta_1 x^i}\\right)^2 \\\\\n\n& = \\frac{1}{2n} * 2 \\sum_{i = 1}^n \\left({y^i - \\beta_0 - \\beta_1 x^i}\\right) \\left({-1}\\right) \\\\\n\n& = \\frac{1}{n} \\sum_{i = 1}^n \\left({e^i}\\right) \\left({-1}\\right) \\\\\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "Introduction_to_Tensors.html#updating-derivatives",
    "href": "Introduction_to_Tensors.html#updating-derivatives",
    "title": "Introduction to Tensors",
    "section": "Updating Derivatives",
    "text": "Updating Derivatives\n\\[\\beta^{\\text{new}} = \\beta^{\\text{old}} - \\alpha\\frac{\\partial L}{\\partial \\beta}\\]\n\\[\\beta_1^{\\text{new}} = \\beta_1^{\\text{old}} - \\alpha\\frac{\\partial L}{\\partial \\beta_1}\\]\n\\[\\beta_0^{\\text{new}} = \\beta_0^{\\text{old}} - \\alpha\\frac{\\partial L}{\\partial \\beta_0}\\]"
  },
  {
    "objectID": "Introduction_to_Tensors.html#pytorch-example",
    "href": "Introduction_to_Tensors.html#pytorch-example",
    "title": "Introduction to Tensors",
    "section": "PyTorch example",
    "text": "PyTorch example\n\nInitialize Data\n\nimport torch\nimport matplotlib.pyplot as plt\n\nw = torch.tensor(-5.0, requires_grad=True)\nb = torch.tensor(0.0, requires_grad=True)\nX = torch.linspace(-3,3,steps = 10).view(-1,1)\ny = -3 * X + 2 + torch.randn(size = X.size())\n\nplt.plot(X.numpy(), y.numpy(),\"ob\")\nplt.grid()\n\n\n\nPerform Prediction\n\ndef forward(x):\n    return w*x + b\n\ndef criterion(yhat, y):\n    return torch.mean((yhat-y)**2)\n\nypred = forward(X)\nprint(criterion(ypred, y))\n\nplt.plot(X.numpy(), y.numpy(),\"ob\")\nplt.plot(X.numpy(), ypred.data.numpy(),\"-r\")\nplt.grid()\n\n\n\nGradient Descent\n\nlr = 0.1\ncost = []\nfor i in range(10):\n    ypred = forward(X)\n    loss = criterion(ypred, y)\n    loss.backward()\n    w.data = w.data - lr*w.grad.data\n    b.data = b.data - lr*b.grad.data\n    w.grad.data.zero_()\n    b.grad.data.zero_()\n    cost.append(loss.item())\n\nprint(loss)\nprint(w.grad)\nprint(b.grad)\nplt.plot(X.numpy(), y.numpy(),\"ob\")\nplt.plot(X.numpy(), ypred.data.numpy(),\"-r\")\nplt.grid()\n\nplt.figure()\nplt.plot(cost, \"-*b\")\nplt.grid()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss value\")\n\n\n\nStochastic Gradient Descent\n\nlr = 0.01\ncost = []\nfor i in range(10):\n    total = 0 \n    for x_,y_ in zip(X,y):\n        ypred = forward(x_)\n        loss = criterion(ypred, y_)\n        loss.backward()\n        w.data = w.data - lr*w.grad.data\n        b.data = b.data - lr*b.grad.data\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n        total += loss.item()\n    cost.append(total)\nprint(loss)\nprint(w.grad)\nprint(b.grad)\n\nplt.figure()\nplt.plot(cost, \"-*b\")\nplt.grid()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss value\")\n\n\n\nDataset Loader Module\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Data(Dataset):\n    def __init__(self, X, y):\n        self.x = X\n        self.y = y\n        self.len = X.shape[0]\n    \n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n\n    def __len__(self):\n        return self.len\n\ndataset = Data(X,y)\nx,y = dataset[:3]\nprint(x)\nprint(y)\n\n\n\nMiniBatch Gradient Descent\n\ntrainloader = DataLoader(dataset=dataset, batch_size=5)\nlr = 0.1\ncost = []\nfor i in range(10):\n    total = 0 \n    for x_,y_ in trainloader:\n        ypred = forward(x_)\n        loss = criterion(ypred, y_)\n        loss.backward()\n        w.data = w.data - lr*w.grad.data\n        b.data = b.data - lr*b.grad.data\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n        total += loss.item()\n    cost.append(total)\nprint(loss)\nprint(w.grad)\nprint(b.grad)\n\n\nplt.figure()\nplt.plot(cost, \"-*b\")\nplt.grid()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss value\")"
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#multiple-linear-regression",
    "href": "Deep Learning for Tabular Data.html#multiple-linear-regression",
    "title": "Deep Learning for Tabular Data",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\n\nLinear Regression as Deep Learning Architecture\n\n\n\\[\\hat{y}^i = w_1x_1^i + w_2x_2^i + ... + w_kx_k^i + b\\]\n\\[\\hat{y} = \\mathbf{xw} + b\\]\n\\[\\mathbf{\\hat{y}} = \\mathbf{Xw} + b\\]"
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#pytorch-implementation",
    "href": "Deep Learning for Tabular Data.html#pytorch-implementation",
    "title": "Deep Learning for Tabular Data",
    "section": "Pytorch Implementation",
    "text": "Pytorch Implementation\n\nimport torch\nfrom torch.nn import Linear\n\n\ntorch.manual_seed(1)\nmdl = Linear(in_features=2,out_features=1)\nlist(mdl.parameters())\nX = torch.rand(size=(3,2))\nprint(mdl(X))\n\ntensor([[ 0.1215],\n        [ 0.0304],\n        [-0.0400]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nCustom Module\nIn PyTorch it is customary to construct custom modules\n\nimport torch.nn as nn\n\nclass LR(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LR, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    def forward(self, x):\n        return self.linear(x)"
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#cost-function",
    "href": "Deep Learning for Tabular Data.html#cost-function",
    "title": "Deep Learning for Tabular Data",
    "section": "Cost Function",
    "text": "Cost Function\n\\[ L(\\mathbf{w},b) =  \\frac{1}{n}\\sum_{i = 1}^n \\left({y^i - \\hat{y}^i}\\right)^2 \\] \\[ L(\\mathbf{w},b) =  \\frac{1}{n}\\sum_{i = 1}^n \\left({y^i - \\mathbf{x^iw}-b}\\right)^2 \\]"
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#initialize-data",
    "href": "Deep Learning for Tabular Data.html#initialize-data",
    "title": "Deep Learning for Tabular Data",
    "section": "Initialize Data",
    "text": "Initialize Data\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Data(Dataset):\n    def __init__(self):\n        self.x = torch.zeros(20,2)\n        self.x[:,0] = torch.arange(-1,1,0.1)\n        self.x[:,1] = torch.arange(-1,1,0.1)\n        self.w = torch.tensor([[1.0],[1.0]])\n        self.b = 1\n        self.f = torch.mm(self.x, self.w) + self.b\n        self.y = self.f + 0.1*torch.randn(self.f.shape)\n        self.len = self.x.shape[0]\n    \n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n\n    def __len__(self):\n        return self.len\n\nThis code defines the Data class, which inherits from the Dataset class in PyTorch. The constructor (__init__) initializes various attributes of the dataset.\n\nself.x[:, 0] = torch.arange(-1, 1, 0.1) and self.x[:, 1] = torch.arange(-1, 1, 0.1): It initializes a 2-dimensional tensor self.x with values ranging from -1 to 1 with a step of 0.1 in both dimensions.\nself.w = torch.tensor([[1.0], [1.0]]): It initializes a weight tensor self.w with values [1.0] in both dimensions.\nself.b = 1: It initializes a bias term self.b with a value of 1.\nself.f = torch.mm(self.x, self.w) + self.b: It calculates the linear function self.f using matrix multiplication (torch.mm) between self.x and self.w, and then adding the bias term self.b.\nself.y = self.f + 0.1 * torch.randn(self.f.shape): It adds random noise to the calculated function self.f to create the target values self.y. This simulates a scenario where real-world data may have some level of noise.\nself.len = self.x.shape[0]: It sets the length of the dataset (self.len) based on the number of rows in self.x."
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#initializing-model-parameters",
    "href": "Deep Learning for Tabular Data.html#initializing-model-parameters",
    "title": "Deep Learning for Tabular Data",
    "section": "Initializing Model Parameters",
    "text": "Initializing Model Parameters\n\nfrom torch import optim\ndata = Data()\ncriterion = nn.MSELoss()\ntrainloader = DataLoader(dataset=data, batch_size=2)\nmdl = LR(input_size=2,output_size=1)\noptimizer = optim.SGD(mdl.parameters(), lr = 0.1)\n\n\nAn instance of the Data class is created (data).\nThe mean squared error loss (criterion) is instantiated using nn.MSELoss().\nA DataLoader (trainloader) is created to handle batches of size 2 during training.\nAn instance of the linear regression model (mdl) is created using the LR class with an input size of 2 and an output size of 1.\nAn SGD optimizer that will update the parameters of the mdl during training, using a learning rate of 0.1 is created. This optimizer will be used in the training loop to update the model parameters based on the computed gradients during backpropagation."
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#training-model",
    "href": "Deep Learning for Tabular Data.html#training-model",
    "title": "Deep Learning for Tabular Data",
    "section": "Training Model",
    "text": "Training Model\n\n\n\ncost =  []\nfor epoch in range(100):\n    for x,y in trainloader:\n        yhat = mdl(x)\n        loss = criterion(yhat,y)\n        cost.append(loss.data)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(cost)\nplt.grid()\n\n\n\n\n\n\n\nfor x, y in trainloader:: This inner loop iterates over batches of data from the trainloader. Each iteration provides a batch of input (x) and target (y) tensors.\nyhat = mdl(x): This line computes the model’s predictions (yhat) given the input (x) using the forward pass of the model.\nloss = criterion(yhat, y): This calculates the loss using the mean squared error (MSE) criterion. The difference between the predicted values (yhat) and the actual target values (y) is computed and squared, and the mean of these squared differences is taken.\ncost.append(loss): The calculated loss for the current batch is appended to the cost list. This allows you to track the change in loss during training.\noptimizer.zero_grad(): This resets the gradients of the model parameters to zero. Gradients accumulate by default in PyTorch, so it’s necessary to clear them before computing new gradients.\nloss.backward(): This computes the gradients of the model parameters with respect to the loss using backpropagation.\noptimizer.step(): This updates the model parameters using the optimizer. The optimizer uses the computed gradients to perform a step in the direction that minimizes the loss."
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#linear-regression-with-multiple-outputs",
    "href": "Deep Learning for Tabular Data.html#linear-regression-with-multiple-outputs",
    "title": "Deep Learning for Tabular Data",
    "section": "Linear Regression with Multiple Outputs",
    "text": "Linear Regression with Multiple Outputs\n\n\n\nLinear Regression with Multiple Outputs\n\n\n\\[\\mathbf{y}_{nxk} = \\mathbf{X}_{nxm}\\mathbf{W}_{mxk} + \\mathbf{b}_{1xk}\\]\n\ntorch.manual_seed(1)\nmdl = Linear(in_features=2,out_features=2)\nlist(mdl.parameters())\nX = torch.rand(size=(3,2))\nprint(mdl(X))\n\ntensor([[-0.7564,  0.6200],\n        [-0.5951,  0.4916],\n        [-0.5967,  0.5106]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\\[ L(\\mathbf{w,b}) =  \\frac{1}{n}\\sum_{i = 1}^n \\left({||y^i - \\mathbf{x^iw}+b}||\\right)^2 \\]"
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#logistic-regressipn",
    "href": "Deep Learning for Tabular Data.html#logistic-regressipn",
    "title": "Deep Learning for Tabular Data",
    "section": "Logistic Regressipn",
    "text": "Logistic Regressipn\n\n\n\nLogistic Regression\n\n\n\\[\\hat{z}^i = w_1x^1 + w_2x^2 + ... + w_kx^k + b\\]\n\\[a^i = \\frac{1}{1 + e^{z^i}}\\]\n\\[\ny^i = \\begin{cases}\n1 & \\text{if } a^i &gt; 0.5 \\\\\n0 & \\text{if } a^i \\leq 0.5\n\\end{cases}\n\\]"
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#logistic-regression-in-pytorch",
    "href": "Deep Learning for Tabular Data.html#logistic-regression-in-pytorch",
    "title": "Deep Learning for Tabular Data",
    "section": "Logistic Regression in PyTorch",
    "text": "Logistic Regression in PyTorch\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, input_size):\n        super(LogisticRegression, self).__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, x):\n        z = self.linear(x)\n        a = torch.sigmoid(z)\n        return a\n\n\nself.linear = nn.Linear(input_size, 1): This line creates a linear layer (nn.Linear) as a member of the class. It represents the linear transformation (weight * input + bias) with input_size input features and 1 output feature, which is typical for logistic regression.\nz = self.linear(x): This line applies the linear transformation to the input x using the weights and biases defined in the self.linear layer. The result is stored in z.\na = torch.sigmoid(z): The logistic sigmoid function (torch.sigmoid) is applied to the linear output z. This operation squashes the values between 0 and 1, which is common in logistic regression, providing the probability of belonging to the positive class."
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#bernoulli-distribution",
    "href": "Deep Learning for Tabular Data.html#bernoulli-distribution",
    "title": "Deep Learning for Tabular Data",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\nThe Bernoulli distribution is a discrete probability distribution for a random variable that can take only two possible outcomes, usually labeled as 0 and 1. It’s commonly used to model binary events, where 0 might represent failure or a “no” outcome, and 1 represents success or a “yes” outcome.\nThe probability mass function (PMF) of a Bernoulli distribution is defined as follows:\n\\[P(X = k) =\n\\begin{cases}\np & \\text{if } k = 1 \\\\\nq = 1 - p & \\text{if } k = 0\n\\end{cases}\n\\]\nHere, \\(p\\) is the probability of success (getting a 1), and \\(q\\) is the probability of failure (getting a 0). The distribution is parameterized by \\(p\\), and it’s often written as \\(Bernoulli(p)\\)."
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#maximum-likelihood-estimation-mle",
    "href": "Deep Learning for Tabular Data.html#maximum-likelihood-estimation-mle",
    "title": "Deep Learning for Tabular Data",
    "section": "Maximum Likelihood Estimation (MLE)",
    "text": "Maximum Likelihood Estimation (MLE)\nMaximum Likelihood Estimation is a method used for estimating the parameters of a statistical model. The idea is to find the values of the model parameters that maximize the likelihood function, which measures how well the model explains the observed data.\nGiven a set of observations \\(x_1, x_2, ..., x_n\\) assumed to be independent and identically distributed (i.i.d.) according to some probability distribution with parameters \\(\\theta\\), the likelihood function is defined as the product of the probability density (or mass) functions of the individual observations:\n\\[L(\\theta | x_1, x_2, ..., x_n) = P(x_1|\\theta) \\times P(x_2|\\theta) \\times \\ldots \\times P(x_n|\\theta)\\]\nThe goal of MLE is to find the values of \\(\\theta\\) that maximize this likelihood function. In practice, it is often more convenient to work with the log-likelihood function (logarithm of the likelihood function), denoted as \\(l(\\theta)\\), as it simplifies calculations and does not change the location of the maximum:\n\\[l(\\theta | x_1, x_2, ..., x_n) = \\log L(\\theta | x_1, x_2, ..., x_n)\\]\nFinding the maximum of \\(l(\\theta)\\) is equivalent to finding the maximum of \\(L(\\theta)\\), and this is often done using optimization techniques."
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#maximum-likelihood-estimation-of-bernouilli-distribution",
    "href": "Deep Learning for Tabular Data.html#maximum-likelihood-estimation-of-bernouilli-distribution",
    "title": "Deep Learning for Tabular Data",
    "section": "Maximum Likelihood Estimation of Bernouilli Distribution",
    "text": "Maximum Likelihood Estimation of Bernouilli Distribution\nWe want to estimate the probability \\(p\\) of getting heads (success) in a single flip, assuming a Bernoulli distribution.\nSuppose we conduct an experiment where we flip a coin 10 times and observe the following outcomes: 1, 0, 1, 1, 0, 1, 1, 0, 0, 1 (1 representing heads, 0 representing tails).\nThe likelihood function for a Bernoulli distribution is given by:\n\\(L(p | x_1, x_2, ..., x_n) = p^{x_1} \\cdot (1-p)^{x_2} \\cdot \\ldots \\cdot p^{x_n}\\)\nIn this case, the likelihood function becomes:\n\\(L(p) = p \\cdot (1-p) \\cdot p \\cdot p \\cdot (1-p) \\cdot p \\cdot p \\cdot (1-p) \\cdot (1-p) \\cdot p\\)\nNow, taking the log-likelihood function:\n\\(l(p) = \\log L(p) = \\log(p) + \\log(1-p) + \\log(p) + \\log(p) + \\log(1-p) + \\log(p) + \\log(p) + \\log(1-p) + \\log(1-p) + \\log(p)\\)\nTo find the maximum likelihood estimate (MLE) of \\(p\\), we differentiate \\(l(p)\\) with respect to \\(p\\), set the result to zero, and solve for \\(p\\). However, in practice, optimization libraries are often used to find the maximum.\nLet’s perform a simplified calculation to illustrate:\n\\(\\frac{dl}{dp} = \\frac{1}{p} - \\frac{1}{1-p} + \\frac{1}{p} + \\frac{1}{p} - \\frac{1}{1-p} + \\frac{1}{p} + \\frac{1}{p} - \\frac{1}{1-p} - \\frac{1}{1-p} + \\frac{1}{p}\\)\nSetting the derivative to zero, we get:\n\\(\\frac{1}{p} - \\frac{1}{1-p} + \\frac{1}{p} + \\frac{1}{p} - \\frac{1}{1-p} + \\frac{1}{p} + \\frac{1}{p} - \\frac{1}{1-p} - \\frac{1}{1-p} + \\frac{1}{p} = 0\\)\nSolving this equation would give the MLE for \\(p\\).\nIn practice, using PyTorch, you might use an optimizer like stochastic gradient descent to find the maximum of the log-likelihood function. The result would be the MLE for \\(p\\)."
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#binary-cross-entropy",
    "href": "Deep Learning for Tabular Data.html#binary-cross-entropy",
    "title": "Deep Learning for Tabular Data",
    "section": "Binary Cross-entropy",
    "text": "Binary Cross-entropy\nBinary Cross-Entropy, often referred to as log loss, is a loss function commonly used in binary classification problems. It measures the difference between the true binary labels and the predicted probabilities for a binary classification task. The formula for binary cross-entropy for a single observation is:\n\\[L(y, \\hat{y}) = - (y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}))\\]\n\n\\(L(y, \\hat{y})\\): Binary cross-entropy loss for a single observation.\n\\(y\\): The true label (ground truth), which is either 0 or 1.\n\\(\\hat{y}\\): The predicted probability that the instance belongs to class 1."
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#logistic-regression-pytorch",
    "href": "Deep Learning for Tabular Data.html#logistic-regression-pytorch",
    "title": "Deep Learning for Tabular Data",
    "section": "Logistic Regression : PyTorch",
    "text": "Logistic Regression : PyTorch\n\ndata = Data()\ncriterion = nn.BCELoss()\ntrainloader = DataLoader(dataset=data, batch_size=2)\nmdl = LR(input_size=2,output_size=1)\noptimizer = optim.SGD(mdl.parameters(), lr = 0.1)"
  },
  {
    "objectID": "Deep Learning for Tabular Data.html#logistic-regression",
    "href": "Deep Learning for Tabular Data.html#logistic-regression",
    "title": "Deep Learning for Tabular Data",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\n\nLogistic Regression\n\n\n\\[\\hat{z}^i =  w_1x_1^i + w_2x_2^i + ... + w_kx_k^i + b\\]\n\\[a^i = \\frac{1}{1 + e^{z^i}}\\]\n\\[\ny^i = \\begin{cases}\n1 & \\text{if } a^i &gt; 0.5 \\\\\n0 & \\text{if } a^i \\leq 0.5\n\\end{cases}\n\\]"
  },
  {
    "objectID": "Deep_Learning_with_PyTorch.html",
    "href": "Deep_Learning_with_PyTorch.html",
    "title": "Untitled",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt\n\n\nx = torch.linspace(-10,10,100)\ny = torch.nn.Sigmoid()(x)\n\nplt.plot(x.numpy(), y.numpy())"
  },
  {
    "objectID": "Deep_Learning_with_PyTorch.html#activation-functions",
    "href": "Deep_Learning_with_PyTorch.html#activation-functions",
    "title": "Untitled",
    "section": "Activation Functions",
    "text": "Activation Functions\nSigmoid Activation Function\n\\[\\sigma(x) = \\frac{1}{1 + \\exp(-x)}\\]"
  },
  {
    "objectID": "Deep_Learning_with_PyTorch.html#deep-networks",
    "href": "Deep_Learning_with_PyTorch.html#deep-networks",
    "title": "Untitled",
    "section": "Deep Networks",
    "text": "Deep Networks\n\n\n\nMulti Layer Perceptron\n\n\n\nimport torch.nn as nn\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H1, H2, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H1)\n        self.linear2 = nn.Linear(H1, H2)\n        self.linear3 = nn.Linear(H2, D_out)\n    \n    # Prediction\n    def forward(self, x):\n        x = torch.tanh(self.linear1(x))\n        x = torch.tanh(self.linear2(x))\n        x = self.linear3(x)\n        return x"
  },
  {
    "objectID": "Deep_Learning_with_PyTorch.html#deeper-networks",
    "href": "Deep_Learning_with_PyTorch.html#deeper-networks",
    "title": "Untitled",
    "section": "Deeper Networks",
    "text": "Deeper Networks\n\nThe nn.ModuleList class in PyTorch provides a convenient way to automate the process of creating a DNN with an arbitrary number of layers.\nTo use nn.ModuleList, we first create a list of the layer sizes.\n\n\nclass Net(nn.Module):\n    def __init__(self, Layers):\n        super(Net, self).__init__()\n        self.hidden = nn.ModuleList()\n        for in_size, out_size in zip(Layers[:-1], Layers[1:]):\n            self.hidden.append(nn.Linear(in_size, out_size))\n    \n    def forward(self, x):\n        L = len(self.hidden)\n        for i, layer in enumerate(self.hidden):\n            x = layer(x)\n            if i &lt; L-1:\n                x = torch.relu(x)\n        return x"
  },
  {
    "objectID": "Deep_Learning_with_PyTorch.html#dropout",
    "href": "Deep_Learning_with_PyTorch.html#dropout",
    "title": "Untitled",
    "section": "Dropout",
    "text": "Dropout\n\nIt involves randomly dropping out (deactivating) a certain percentage of neurons during training.\nDropout is a regularization technique used to prevent overfitting in deep neural networks.\n\n\nimport torch\nimport torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self, p):\n        super(Net, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        x = self.dropout(x)\n        x = self.fc1(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x"
  },
  {
    "objectID": "Deep_Learning_with_PyTorch.html#weight-initialization",
    "href": "Deep_Learning_with_PyTorch.html#weight-initialization",
    "title": "Untitled",
    "section": "Weight Initialization",
    "text": "Weight Initialization\n\nWeight initialization is a crucial step in training neural networks.\nProper initialization can help improve convergence, prevent vanishing/exploding gradients, and enhance overall performance.\nRandom initialization breaks symmetry among neurons.\nPrevents all neurons from learning the same features and promotes diverse representations."
  },
  {
    "objectID": "Deep_Learning_with_PyTorch.html#weight-initialization-1",
    "href": "Deep_Learning_with_PyTorch.html#weight-initialization-1",
    "title": "Untitled",
    "section": "Weight Initialization",
    "text": "Weight Initialization\nUniform Initialization\nInitialize weights by sampling from a uniform distribution.\n\ntensor = nn.Linear(in_features=10, out_features=5)  # Example linear layer\nnn.init.uniform_(tensor.weight, a=-1, b=1)\n\nParameter containing:\ntensor([[-0.2195,  0.1444,  0.2317,  0.4608, -0.9648, -0.0273, -0.3420, -0.0551,\n          0.3293, -0.4923],\n        [ 0.7167, -0.7186,  0.2357,  0.4891,  0.0823,  0.2721, -0.9871,  0.1166,\n         -0.2548, -0.1073],\n        [ 0.7706, -0.4560,  0.1785,  0.0071,  0.5740, -0.0977, -0.7855, -0.6013,\n          0.6511,  0.4530],\n        [-0.1009, -0.5508, -0.0140, -0.2820,  0.3267, -0.4588,  0.8212, -0.4839,\n          0.6538,  0.8569],\n        [-0.1919, -0.7329,  0.3365,  0.9854, -0.6871,  0.2606, -0.0737,  0.7884,\n          0.3673,  0.6822]], requires_grad=True)\n\n\nXavier Initialization\n\\(w \\sim \\mathcal{U}\\left[ -\\sqrt{\\frac{6}{f_{in} + f_{out}}}, \\sqrt{\\frac{6}{f_{in} + f_{out}}} \\right]\\)\nwhere:\n\n\\(w\\) represents the weight\n\\(\\mathcal{U}\\) denotes the uniform distribution\n\\(f_{in}\\) is the number of incoming connections (fan-in)\n\\(f_{out}\\) is the number of outgoing connections (fan-out)"
  },
  {
    "objectID": "Deep_Learning_with_PyTorch.html#uniform-initialization",
    "href": "Deep_Learning_with_PyTorch.html#uniform-initialization",
    "title": "Untitled",
    "section": "Uniform Initialization",
    "text": "Uniform Initialization\nInitialize weights by sampling from a uniform distribution.\n\ntensor = nn.Linear(in_features=10, out_features=5)  # Example linear layer\nnn.init.uniform_(tensor.weight, a=-1, b=1)\n\nParameter containing:\ntensor([[-0.3937,  0.1210, -0.9787,  0.8851,  0.3756,  0.8710, -0.8725,  0.6768,\n         -0.7686,  0.3757],\n        [-0.9251,  0.9058, -0.3840,  0.4064, -0.5976,  0.7266,  0.5879,  0.6258,\n          0.0729, -0.6791],\n        [-0.7663, -0.1871,  0.5096,  0.7236, -0.9915,  0.6802, -0.8710,  0.7848,\n         -0.5713,  0.7554],\n        [ 0.1959, -0.4829, -0.5761, -0.8467,  0.8185,  0.8296, -0.3859, -0.4307,\n          0.3862, -0.6496],\n        [ 0.6510,  0.3643, -0.2506,  0.4830,  0.0268,  0.1118,  0.6920,  0.4659,\n         -0.8387, -0.7915]], requires_grad=True)"
  },
  {
    "objectID": "Deep_Learning_with_PyTorch.html#xavier-initialization",
    "href": "Deep_Learning_with_PyTorch.html#xavier-initialization",
    "title": "Untitled",
    "section": "Xavier Initialization",
    "text": "Xavier Initialization\n\\(w \\sim \\mathcal{U}\\left[ -\\sqrt{\\frac{6}{f_{in} + f_{out}}}, \\sqrt{\\frac{6}{f_{in} + f_{out}}} \\right]\\)\nwhere:\n\n\\(w\\) represents the weight\n\\(\\mathcal{U}\\) denotes the uniform distribution\n\\(f_{in}\\) is the number of incoming connections (fan-in)\n\\(f_{out}\\) is the number of outgoing connections (fan-out)\n\n\ntensor = nn.Linear(in_features=10, out_features=5)  # Example linear layer\nnn.init.xavier_uniform_(tensor.weight)\n\nParameter containing:\ntensor([[ 0.4524,  0.2749,  0.3535, -0.4792,  0.5283, -0.1491, -0.3514, -0.3441,\n         -0.1817,  0.2032],\n        [ 0.0065, -0.0767,  0.2320, -0.0278, -0.5130,  0.4101, -0.0479,  0.5184,\n         -0.4542, -0.4870],\n        [ 0.3766, -0.1222, -0.0852, -0.4587, -0.5463,  0.5804,  0.6290, -0.3597,\n         -0.3914,  0.3897],\n        [ 0.3789, -0.2772, -0.4392, -0.2069, -0.3931, -0.2855,  0.3502,  0.2395,\n          0.5325,  0.0517],\n        [ 0.3958, -0.0679, -0.1819,  0.0448,  0.1934,  0.2835, -0.3714, -0.5550,\n          0.2784, -0.4476]], requires_grad=True)"
  },
  {
    "objectID": "Deep_Learning_with_PyTorch.html#he-initialization",
    "href": "Deep_Learning_with_PyTorch.html#he-initialization",
    "title": "Untitled",
    "section": "He Initialization",
    "text": "He Initialization\n\\(w \\sim \\mathcal{N}\\left(0, -\\sqrt{\\frac{2}{f_{in}}}\\right)\\)\n\ntensor = nn.Linear(in_features=10, out_features=5)  # Example linear layer\nnn.init.kaiming_uniform_(tensor.weight, mode='fan_in', nonlinearity='relu')\n\nParameter containing:\ntensor([[-0.0321,  0.1310, -0.5187,  0.6252, -0.7461, -0.1692, -0.6839,  0.6301,\n          0.1479,  0.1805],\n        [ 0.5695, -0.3742, -0.4610, -0.3234, -0.3796, -0.2038, -0.3788, -0.0462,\n         -0.1786,  0.0951],\n        [-0.1256, -0.3321,  0.2212,  0.6909, -0.0901, -0.7604,  0.4656,  0.7615,\n         -0.3997, -0.7632],\n        [-0.6621,  0.1149, -0.6051, -0.5598, -0.0878, -0.7648, -0.4447,  0.4021,\n          0.2565, -0.0673],\n        [ 0.2964, -0.6447, -0.2196,  0.0640, -0.6823, -0.1736, -0.1809,  0.2535,\n         -0.6424,  0.7558]], requires_grad=True)"
  }
]